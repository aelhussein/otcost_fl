import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import os
from typing import Optional, List, Dict
import matplotlib.lines as mlines 

def pivot_results_comparison(results_df: pd.DataFrame) -> pd.DataFrame:
    """
    Pivots the results table to compare the primary metric across parameter sets.

    Args:
        results_df: The combined DataFrame returned by PipelineRunner.

    Returns:
        A pivoted DataFrame.
    """
    if results_df is None or results_df.empty:
        return pd.DataFrame()

    # Create a unified 'Primary_Metric' column based on OT_Method
    # This assumes you know the main metric for each method
    def get_primary_metric(row):
        if row['OT_Method'] == 'feature_error':
            return row['FeatureErrorOT_Cost'] # Lower is better
        elif row['OT_Method'] == 'decomposed':
            return row['Decomposed_ConditionalOT'] # Lower is better? (Depends on interpretation)
        elif row['OT_Method'] == 'fixed_anchor':
            return row['FixedAnchor_TotalCost']
        else:
            return np.nan

    temp_df = results_df.copy()
    temp_df['Primary_Metric'] = temp_df.apply(get_primary_metric, axis=1)

    # Define columns to keep alongside the pivoted values (optional)
    index_cols = ['Cost', 'Local_Final_Loss', 'FedAvg_Final_Loss', 'Loss_Delta']
    # Ensure index_cols are present
    index_cols = [col for col in index_cols if col in temp_df.columns]


    try:
        # Pivot table: Costs as rows, Parameter Sets as columns, Primary Metric as values
        pivot_df = pd.pivot_table(temp_df,
                                  index=index_cols,
                                  columns='Param_Set_Name',
                                  values='Primary_Metric')
        return pivot_df.reset_index() # Reset index to make index_cols regular columns
    except Exception as e:
        print(f"Could not pivot table: {e}")
        return pd.DataFrame()


def plot_ot_metrics_vs_perf_delta(
    results_df: pd.DataFrame,
    # Removed param_sets_to_plot, filtering can be done before calling
    main_title: Optional[str] = None,
    save_path: Optional[str] = None
    ):
    """
    Generates a figure with one subplot per OT Method found in the DataFrame.
    Each subplot shows the primary metric for that method vs. Loss_Delta_pct.
    Lines within each subplot are colored by parameter set name.

    Args:
        results_df: DataFrame generated by the pipeline (long format).
        main_title: Optional main title for the entire figure.
        save_path: Optional path to save the plot figure. If None, displays the plot.
    """
    # --- Input Validation & Setup ---
    if results_df is None or results_df.empty:
        warnings.warn("Input DataFrame is empty or None. Cannot generate plot.")
        return

    required_cols = ['Cost', 'OT_Method', 'Param_Set_Name',
                     'Local_Final_Loss', 'FedAvg_Final_Loss', 'Loss_Delta']
    if not all(col in results_df.columns for col in required_cols):
        warnings.warn(f"Input DataFrame missing required columns: {required_cols}. Cannot plot.")
        return

    plot_df = results_df.copy()

    # --- Calculate Percentage Delta ---
    # Avoid division by zero or near-zero
    plot_df['Loss_Delta_pct'] = np.where(
        np.abs(plot_df['Local_Final_Loss']) > 1e-9,
        (plot_df['Loss_Delta'] / plot_df['Local_Final_Loss']) * 100.0,
        np.nan # Assign NaN if local loss is too small
    )
    y_var = 'Loss_Delta_pct'
    y_label = "Performance Delta (%) [(Local - FedAvg)/Local]"

    # --- Define Primary Metric per Method ---
    # Map OT_Method value to the column holding its primary result
    primary_metric_map = {
        'feature_error': 'FeatureErrorOT_Cost',
        'decomposed': 'Decomposed_CombinedScore',
        'fixed_anchor': 'FixedAnchor_TotalCost' # Changed from TotalCost to SimScore
        # Add new methods here
    }
    # Define corresponding labels for axes
    metric_labels = {
        'feature_error': 'Feature-Error OT Cost',
        'decomposed': 'Decomposed Combined Score',
        'fixed_anchor': 'Fixed Anchor Cost'
        # Add new methods here
    }
     # Indicate if lower is better (True) or higher is better (False) for the metric
    lower_is_better = {
        'feature_error': True,
        'decomposed': True, # Assuming combined score is a cost/distance
        'fixed_anchor': False # Similarity score, higher is better
    }

    # Filter to methods present in the data and defined above
    available_methods = [m for m in plot_df['OT_Method'].unique() if m in primary_metric_map]

    if not available_methods:
        warnings.warn("No defined OT methods found in the DataFrame. Cannot plot.")
        return

    # --- Subplot Setup ---
    num_methods = len(available_methods)
    fig, axes = plt.subplots(1, num_methods, figsize=(7 * num_methods, 6), sharey=True)
    if num_methods == 1:
        axes = [axes] # Ensure axes is iterable even with one subplot

    plot_successful = False
    hue_var = 'Param_Set_Name'

    # --- Plotting Loop (One subplot per method)---
    for i, method_type in enumerate(available_methods):
        ax = axes[i]
        metric_col = primary_metric_map[method_type]
        metric_label = metric_labels.get(method_type, method_type) # Fallback label

        if metric_col not in plot_df.columns:
            warnings.warn(f"Primary metric column '{metric_col}' for method '{method_type}' not found. Skipping subplot.")
            ax.set_title(f"{metric_label}\n(Data Unavailable)", fontsize=14)
            ax.text(0.5, 0.5, "Data Unavailable", ha='center', va='center', transform=ax.transAxes, fontsize=12, color='red')
            continue

        # Filter data for the current method and drop rows with NaN in essential columns
        method_df = plot_df[plot_df['OT_Method'] == method_type].dropna(subset=[metric_col, y_var, hue_var]).copy()

        if method_df.empty:
            warnings.warn(f"No valid data for subplot ('{metric_col}' vs '{y_var}' for method '{method_type}'). Skipping.")
            ax.set_title(f"{metric_label}\n(No Valid Data)", fontsize=14)
            ax.text(0.5, 0.5, "No Valid Data", ha='center', va='center', transform=ax.transAxes, fontsize=12, color='orange')
            continue

        # Convert relevant columns to numeric just in case
        try:
            method_df[metric_col] = pd.to_numeric(method_df[metric_col])
            method_df[y_var] = pd.to_numeric(method_df[y_var])
        except Exception as e:
             warnings.warn(f"Numeric conversion error for subplot '{metric_label}': {e}. Skipping.")
             # ... (add error text to plot as before) ...
             continue

        # Create the plot for this method
        sns.lineplot(
            data=method_df, x=metric_col, y=y_var, hue=hue_var, style=hue_var,
            marker='o', markersize=8, linewidth=2, ax=ax, legend=False # Turn off individual legends
        )

        ax.set_title(f"{metric_label} vs. Perf. Delta (%)", fontsize=14)
        ax.axhline(y=0, linestyle='--', color='black', linewidth=1)
        ax.set_xlabel(metric_label, fontsize=12)
        ax.grid(True, linestyle='--', alpha=0.6)

        # Optional: Adjust x-limits based on metric type (e.g., similarity vs cost)
        # if not lower_is_better.get(method_type, True): # Higher is better (Similarity)
        #    ax.set_xlim(ax.get_xlim()[::-1]) # Reverse x-axis if desired for similarity
        # elif method_type == 'fixed_anchor': # Similarity often 0-1
        #      ax.set_xlim(0.0, 1.05)


        plot_successful = True


    # --- Figure-Level Customization ---
    if not plot_successful:
         warnings.warn("No subplots could be generated.")
         plt.close(fig)
         return

    # Set shared Y-label
    axes[0].set_ylabel(y_label, fontsize=12)

    fig_title = main_title if main_title else "OT Metrics vs. Performance Delta by Parameter Set"
    fig.suptitle(fig_title, fontsize=18, y=0.99) # Adjusted y position slightly

    # --- Create a single figure legend ---
    # Get handles/labels from the first axis that was plotted
    handles, labels = axes[0].get_legend_handles_labels() # Use first axis as reference
    if handles:
        fig.legend(handles, labels, title='Parameter Set',
                   bbox_to_anchor=(1.01, 1), # Position slightly outside top-right
                   loc='upper left',         # Align legend's upper left to anchor
                   ncol=1,                   # Stack vertically
                   borderaxespad=0.)
    else:
        warnings.warn("Could not retrieve handles/labels for figure legend.")

    # Adjust layout
    fig.tight_layout(rect=[0, 0, 0.88, 0.95]) # Make space for legend

    # --- Output ---
    if save_path:
        try:
            save_dir = os.path.dirname(save_path)
            if save_dir and not os.path.exists(save_dir):
                os.makedirs(save_dir)
            fig.savefig(save_path, dpi=300, bbox_inches='tight') # Include legend
            print(f"Plot saved to: {save_path}")
        except Exception as e:
            warnings.warn(f"Failed to save plot to {save_path}: {e}")
        plt.close(fig)
    else:
        plt.show()