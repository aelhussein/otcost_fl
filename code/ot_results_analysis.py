import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import os
from typing import Optional, List, Dict
import matplotlib.lines as mlines 

def pivot_results_comparison(results_df: pd.DataFrame) -> pd.DataFrame:
    """
    Pivots the results table to compare the primary metric across parameter sets.

    Args:
        results_df: The combined DataFrame returned by PipelineRunner.

    Returns:
        A pivoted DataFrame.
    """
    if results_df is None or results_df.empty:
        return pd.DataFrame()

    # Create a unified 'Primary_Metric' column based on OT_Method
    # This assumes you know the main metric for each method
    def get_primary_metric(row):
        if row['OT_Method'] == 'feature_error':
            return row['FeatureErrorOT_Cost'] # Lower is better
        elif row['OT_Method'] == 'decomposed':
            return row['Decomposed_CombinedScore'] # Lower is better
        elif row['OT_Method'] == 'fixed_anchor':
            return row['FixedAnchor_TotalCost']
        elif row['OT_Method'] == 'direct_ot':
            return row['DirectOT_Cost']  # New entry for direct_ot
        else:
            return np.nan

    temp_df = results_df.copy()
    temp_df['Primary_Metric'] = temp_df.apply(get_primary_metric, axis=1)

    # Define columns to keep alongside the pivoted values (optional)
    index_cols = ['Cost', 'Local_Final_Loss', 'FedAvg_Final_Loss', 'Loss_Delta']
    # Ensure index_cols are present
    index_cols = [col for col in index_cols if col in temp_df.columns]


    try:
        # Pivot table: Costs as rows, Parameter Sets as columns, Primary Metric as values
        pivot_df = pd.pivot_table(temp_df,
                                  index=index_cols,
                                  columns='Param_Set_Name',
                                  values='Primary_Metric')
        return pivot_df.reset_index() # Reset index to make index_cols regular columns
    except Exception as e:
        print(f"Could not pivot table: {e}")
        return pd.DataFrame()



def plot_ot_metrics_vs_perf_delta(
    results_df: pd.DataFrame,
    main_title: Optional[str] = None,
    save_path: Optional[str] = None
    ):
    """
    Generates a figure with one subplot per OT Method found in the DataFrame.
    Each subplot shows the primary metric for that method vs. Loss_Delta_pct,
    with its own legend for the Parameter Sets relevant to that method.

    Args:
        results_df: DataFrame generated by the pipeline (long format).
        main_title: Optional main title for the entire figure.
        save_path: Optional path to save the plot figure. If None, displays the plot.
    """
    # --- Input Validation & Setup ---
    if results_df is None or results_df.empty:
        warnings.warn("Input DataFrame is empty or None. Cannot generate plot.")
        return

    required_cols = ['Cost', 'OT_Method', 'Param_Set_Name',
                     'Local_Final_Loss', 'FedAvg_Final_Loss', 'Loss_Delta']
    if not all(col in results_df.columns for col in required_cols):
        missing = [col for col in required_cols if col not in results_df.columns]
        warnings.warn(f"Input DataFrame missing required columns: {missing}. Cannot plot.")
        return

    plot_df = results_df.copy()

    # --- Calculate Percentage Delta ---
    plot_df['Loss_Delta_pct'] = np.where(
        np.abs(plot_df['Local_Final_Loss']) > 1e-9,
        (plot_df['Loss_Delta'] / plot_df['Local_Final_Loss']) * 100.0,
        np.nan
    )
    y_var = 'Loss_Delta_pct'
    y_label = "Performance Delta (%) [(Local - FedAvg)/Local]"

    # --- Define Primary Metric per Method ---
    primary_metric_map = {
        'feature_error': 'FeatureErrorOT_Cost',
        'decomposed': 'Decomposed_CombinedScore',
        'fixed_anchor': 'FixedAnchor_TotalCost',
        'direct_ot': 'DirectOT_Cost'
    }
    metric_labels = {
        'feature_error': 'Feature-Error OT Cost',
        'decomposed': 'Decomposed Combined Score',
        'fixed_anchor': 'Fixed Anchor Total Cost',
        'direct_ot': 'Direct OT Cost'
    }
    # lower_is_better is not directly used for plotting structure now,
    # but kept for potential future use or interpretation.
    lower_is_better = {
        'feature_error': True, 'decomposed': True,
        'fixed_anchor': True, 'direct_ot': True
    }

    available_methods = sorted([m for m in plot_df['OT_Method'].unique() if m in primary_metric_map])

    if not available_methods:
        warnings.warn("No defined OT methods found in the DataFrame. Cannot plot.")
        return

    # --- Subplot Setup ---
    num_methods = len(available_methods)
    # Adjust figsize slightly if many methods, legends take space
    fig_width = max(7 * num_methods, 10) # Ensure minimum width
    fig, axes = plt.subplots(1, num_methods, figsize=(fig_width, 6), sharey=True, squeeze=False)
    axes = axes.flatten()

    plot_successful = False
    hue_var = 'Param_Set_Name'

    # --- Plotting Loop ---
    for i, method_type in enumerate(available_methods):
        ax = axes[i]
        metric_col = primary_metric_map[method_type]
        metric_label = metric_labels.get(method_type, method_type)

        if metric_col not in plot_df.columns:
            warnings.warn(f"Primary metric column '{metric_col}' for method '{method_type}' not found. Skipping subplot.")
            ax.set_title(f"{metric_label}\n(Data Unavailable)", fontsize=14)
            ax.text(0.5, 0.5, "Data Unavailable", ha='center', va='center', transform=ax.transAxes, fontsize=12, color='red')
            continue

        # Filter data *specific to this method*
        method_df = plot_df[plot_df['OT_Method'] == method_type].dropna(subset=[metric_col, y_var, hue_var]).copy()

        if method_df.empty:
            warnings.warn(f"No valid data for subplot ('{metric_col}' vs '{y_var}' for method '{method_type}'). Skipping.")
            ax.set_title(f"{metric_label}\n(No Valid Data)", fontsize=14)
            ax.text(0.5, 0.5, "No Valid Data", ha='center', va='center', transform=ax.transAxes, fontsize=12, color='orange')
            continue

        # Check unique parameter sets for this method
        param_sets_in_subplot = method_df[hue_var].unique()
        if len(param_sets_in_subplot) == 0:
             warnings.warn(f"No unique '{hue_var}' values found for method '{method_type}' after filtering. Skipping plot elements.")
             ax.set_title(f"{metric_label}\n(No Param Sets)", fontsize=14)
             ax.text(0.5, 0.5, "No Param Sets", ha='center', va='center', transform=ax.transAxes, fontsize=12, color='grey')
             continue


        try:
            method_df[metric_col] = pd.to_numeric(method_df[metric_col])
            method_df[y_var] = pd.to_numeric(method_df[y_var])
        except Exception as e:
             warnings.warn(f"Numeric conversion error for subplot '{metric_label}': {e}. Skipping.")
             ax.set_title(f"{metric_label}\n(Conversion Error)", fontsize=14)
             ax.text(0.5, 0.5, "Data Conversion Error", ha='center', va='center', transform=ax.transAxes, fontsize=12, color='purple')
             continue

        # --- Create the plot for this method ---
        try:
            sns.lineplot(
                data=method_df, x=metric_col, y=y_var, hue=hue_var, style=hue_var,
                marker='o', markersize=8, linewidth=2, ax=ax,
                legend='auto' # <<< Let seaborn handle legend creation per axis
            )
            # --- Adjust individual legend ---
            current_legend = ax.get_legend()
            if current_legend:
                current_legend.set_title('Parameter Set')
                plt.setp(current_legend.get_texts(), fontsize='small') # Adjust font size if needed
                plt.setp(current_legend.get_title(), fontsize='small')
            # else: # Should not happen with legend='auto' unless no data plotted
            #     warnings.warn(f"Legend not automatically created for subplot {i} ({method_type})")

        except Exception as plot_err:
             warnings.warn(f"Seaborn lineplot failed for subplot '{metric_label}': {plot_err}. Skipping plot elements.")
             ax.set_title(f"{metric_label}\n(Plotting Error)", fontsize=14)
             ax.text(0.5, 0.5, "Plotting Error", ha='center', va='center', transform=ax.transAxes, fontsize=12, color='brown')
             continue

        ax.set_title(f"{metric_label} vs. Perf. Delta (%)", fontsize=14)
        ax.axhline(y=0, linestyle='--', color='black', linewidth=1)
        ax.set_xlabel(metric_label, fontsize=12)
        ax.grid(True, linestyle='--', alpha=0.6)

        plot_successful = True

    # --- Figure-Level Customization ---
    if not plot_successful:
         warnings.warn("No subplots could be generated.")
         plt.close(fig)
         return

    # Set shared Y-label using the first axis
    axes[0].set_ylabel(y_label, fontsize=12)

    fig_title = main_title if main_title else "OT Metrics vs. Performance Delta by Parameter Set"
    fig.suptitle(fig_title, fontsize=18, y=1.02) # Adjust title position

    # --- Remove Figure Legend Code ---
    # The block creating fig.legend() is removed.

    # Adjust layout to prevent overlap - may need tweaking
    fig.tight_layout()
    # plt.subplots_adjust(top=0.90) # Might be needed if suptitle overlaps axes titles

    # --- Output ---
    if save_path:
        try:
            save_dir = os.path.dirname(save_path)
            if save_dir and not os.path.exists(save_dir):
                os.makedirs(save_dir)
            # Save with bbox_inches='tight' to try and include legends if they extend slightly
            fig.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"Plot saved to: {save_path}")
        except Exception as e:
            warnings.warn(f"Failed to save plot to {save_path}: {e}")
        plt.close(fig)
    else:
        plt.show()